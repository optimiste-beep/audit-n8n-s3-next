# Audit-n8n-s3-next

## ğŸš€ Project Overview

Audit-n8n-s3-next is an intelligent document processing and Q&A automation workflow built with n8n. The system automatically processes audit-related documents and questionnaires, creates searchable knowledge bases, and generates comprehensive Q&A reports using advanced RAG (Retrieval-Augmented Generation) technology.

## ğŸ“‹ Project Scope

This workflow enables organizations to:
- **Automate Document Processing**: Process PDF documents and Excel questionnaires from S3 storage
- **Create Knowledge Bases**: Generate vector embeddings for intelligent document search
- **Generate Q&A Reports**: Automatically answer questionnaire items using processed documents
- **Multi-tenant Support**: Handle multiple organizations, users, and sessions simultaneously
- **Export Results**: Generate Excel reports and upload to S3 and Google Sheets

### Key Features
- âœ… **Multi-user isolation**: Complete data separation between organizations and users
- âœ… **Intelligent deduplication**: Prevents reprocessing of already embedded documents
- âœ… **Real-time monitoring**: Cron-based polling for new documents
- âœ… **Comprehensive logging**: Detailed processing status and error tracking
- âœ… **Multiple output formats**: Excel, CSV, and Google Sheets integration
- âœ… **Advanced RAG processing**: Context-aware question answering using vector databases

## ğŸ—ï¸ System Architecture

### Core Components

```mermaid
graph TB
    S3[S3 Storage] --> Monitor[Cron Monitor]
    Monitor --> Parse[XML Parser]
    Parse --> Group[Session Grouper]
    Group --> Filter[Ready Filter]
    Filter --> Dedup[Deduplication]
    Dedup --> Split{File Splitter}
    
    Split -->|PDFs| PdfPipe[PDF Pipeline]
    Split -->|Excel| ExcelPipe[Excel Pipeline]
    
    PdfPipe --> Extract[Text Extraction]
    Extract --> Embed[Vector Embedding]
    Embed --> Store[Pinecone Storage]
    
    ExcelPipe --> Questions[Question Extraction]
    Questions --> RAG[RAG Agent]
    Store --> RAG
    RAG --> Format[Answer Formatting]
    Format --> Results[Results Aggregation]
    
    Results --> S3Out[S3 Upload]
    Results --> Sheets[Google Sheets]
```

### Technology Stack

- **Orchestration**: n8n workflow automation
- **Vector Database**: Pinecone for document embeddings
- **LLM**: Google Gemini 2.0 Flash for Q&A generation
- **Embeddings**: Cohere embedding model
- **Storage**: AWS S3 for file management
- **Reporting**: Google Sheets API for real-time results
- **Document Processing**: PDF text extraction and Excel parsing

### Data Flow Architecture

1. **Document Ingestion**: S3 bucket monitoring (`uploadsfromchatbot`)
2. **Session Organization**: Files grouped by `org/user/session` structure
3. **Dual Processing Pipelines**:
   - **PDF Pipeline**: Text extraction â†’ Chunking â†’ Vector embedding â†’ Storage
   - **Excel Pipeline**: Question extraction â†’ RAG processing â†’ Answer generation
4. **Results Consolidation**: Multi-format output generation and distribution

## ğŸš¦ Getting Started

### Prerequisites

Before deploying this workflow, ensure you have:

#### Required Services
- **n8n instance** (self-hosted or cloud)
- **AWS S3 bucket** with appropriate IAM permissions
- **Pinecone account** with vector database
- **Cohere API key** for embeddings
- **Google Cloud Project** with Gemini API access
- **Google Sheets API** credentials (optional)

#### Required n8n Nodes
The workflow uses several specialized n8n nodes:
- `@n8n/n8n-nodes-langchain` (LangChain integration)
- `n8n-nodes-pdf-excel.pdfExcelProcessor` (PDF processing)
- Standard n8n nodes (HTTP, S3, Code, etc.)

### Installation Steps

1. **Clone the Repository**
   ```bash
   git clone https://github.com/yourusername/audit-n8n-s3-next.git
   cd audit-n8n-s3-next
   ```

2. **Import Workflow**
   - Open your n8n instance
   - Navigate to **Workflows** â†’ **Import from File**
   - Upload `AuditN8NWorkflow.json`

3. **Configure Credentials**
   Set up the following credential types in n8n:

   #### AWS S3 Credentials
   ```
   Credential Type: AWS
   Access Key ID: [your-access-key]
   Secret Access Key: [your-secret-key]
   Region: eu-west-2 (or your bucket region)
   ```

   #### Pinecone Credentials
   ```
   Credential Type: Pinecone API
   API Key: [your-pinecone-api-key]
   Environment: [your-pinecone-environment]
   ```

   #### Cohere Credentials
   ```
   Credential Type: Cohere API
   API Key: [your-cohere-api-key]
   ```

   #### Google Gemini Credentials
   ```
   Credential Type: Google PaLM API
   API Key: [your-google-api-key]
   ```

   #### Google Sheets (Optional)
   ```
   Credential Type: Google Sheets OAuth2 API
   [Complete OAuth2 setup process]
   ```

4. **Configure Workflow Parameters**
   
   > ğŸ“ **Important**: Update these key settings before running the workflow
   
   | Parameter | Description | Action Required |
   |-----------|-------------|-----------------|
   | **S3 Bucket Name** | Target bucket for file storage | Update all S3 nodes to use your bucket name |
   | **Pinecone Index** | Vector database index | Ensure index name matches your Pinecone setup |
   | **Cron Schedule** | Monitoring frequency | Adjust timing (default: every 5 seconds) |

### ğŸ“ File Structure Requirements

Your S3 bucket **must** follow this exact hierarchical structure:

```bash
ğŸ“¦ uploadsfromchatbot/
â”œâ”€â”€ ğŸ¢ org1/
â”‚   â”œâ”€â”€ ğŸ‘¤ user1/
â”‚   â”‚   â”œâ”€â”€ ğŸ“Š session001/
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ documents/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ audit_report.pdf
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ financial_statements.pdf
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“‹ questionnaires/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“Š compliance_questions.xlsx
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“ˆ results/
â”‚   â”‚   â”‚       â””â”€â”€ ğŸ“Š [generated files]
â”‚   â”‚   â””â”€â”€ ğŸ“Š session002/
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ documents/
â”‚   â”‚       â””â”€â”€ ğŸ“‹ questionnaires/
â”‚   â””â”€â”€ ğŸ‘¤ user2/
â”‚       â””â”€â”€ ğŸ“Š session003/
â””â”€â”€ ğŸ¢ org2/
    â””â”€â”€ ğŸ‘¤ user3/
        â””â”€â”€ ğŸ“Š session004/
```

> âš ï¸ **Critical**: Sessions are only processed when **both** `documents/` and `questionnaires/` folders contain files

### ğŸ“Š Excel Questionnaire Format

Questionnaires must be `.xlsx` files with this specific structure:

```
ğŸ“‹ Questionnaire Structure Requirements:

âœ… File Type: .xlsx (Excel format)
âœ… Location: /questionnaires/ folder
âœ… Headers: Module names in columns
âœ… Content: Questions in rows
```


## âš¡ Usage

### Starting the Workflow

1. **Activate the Workflow** in n8n
2. The cron trigger will automatically start monitoring your S3 bucket
3. Upload files to S3 following the required structure
4. Monitor execution in the n8n interface

### Processing Flow

1. **File Upload**: Place PDF documents in `/documents/` and Excel questionnaires in `/questionnaires/`
2. **Automatic Detection**: Workflow detects new sessions with both file types
3. **PDF Processing**: Documents are extracted, chunked, and embedded
4. **Q&A Generation**: Questions are answered using the embedded knowledge
5. **Results Export**: Answers are formatted and exported to multiple destinations

### Output Formats

The workflow generates several output files:
- **Excel Report**: `org_user_session_timestamp.xlsx`
- **CSV Export**: `org_user_session_timestamp.csv`
- **JSON Summary**: `summary_org_user_session_timestamp.json`
- **Completion Status**: `completion_org_user_session.txt`
- **Google Sheets**: Live collaborative spreadsheet (if configured)

## ğŸ”§ Configuration

### ğŸ”§ Configuration Options

#### â° Cron Monitoring Settings
```javascript
// ğŸ“ Location: "Cron Trigger - Every 5 Seconds" node
// âš™ï¸ Adjust monitoring frequency based on your needs

schedule: "*/5 * * * * *"   // âš¡ Every 5 seconds (development)
schedule: "*/30 * * * * *"  // ğŸ”„ Every 30 seconds (production)
schedule: "0 */5 * * * *"   // â±ï¸ Every 5 minutes (low traffic)
```

#### ğŸ§  Vector Database Configuration
```javascript
// ğŸ“ Location: "Text Splitter" node
// ğŸ“Š Optimize for your document types

{
  chunkSize: 4000,          // ğŸ“„ Characters per chunk
  chunkOverlap: 400,        // ğŸ”„ Overlap between chunks
  separators: ["\n\n", "\n", " ", ""]  // ğŸ“ Text splitting logic
}
```

#### ğŸ¯ RAG Processing Parameters
```javascript
// ğŸ“ Location: "Session Knowledge Base (Pinecone)" node
// ğŸ” Fine-tune retrieval accuracy

{
  topK: 10,                 // ğŸ“š Number of relevant chunks to retrieve
  scoreThreshold: 0.7,      // ğŸ“ˆ Minimum relevance score
  includeMetadata: true,    // ğŸ“‹ Include document metadata
  includeValues: false      // ğŸ”¢ Include embedding vectors
}
```

#### âš¡ Performance & Scaling
```javascript
// ğŸ“ Location: "S3 objects and group them by session" node
// ğŸš€ Adjust for your scale requirements

const config = {
  MAX_SESSIONS_PER_BATCH: 50,      // ğŸ“Š Sessions processed per run
  MAX_FILES_PER_SESSION: 100,     // ğŸ“ Files per session limit
  BATCH_TIMEOUT_MS: 300000,       // â° 5-minute processing timeout
  RETRY_ATTEMPTS: 3               // ğŸ”„ Retry failed operations
};
```

#### ğŸŒ Multi-tenant Isolation
```javascript
// ğŸ“ Location: Multiple nodes
// ğŸ”’ Ensure complete data separation

const isolation = {
  namespacePattern: "${org}_${user}_${session}",     // ğŸ·ï¸ Pinecone namespace
  s3PathPattern: "${org}/${user}/${session}/",       // ğŸ“ S3 path structure
  resourceGroup: "${org}_processing",                // ğŸ¢ Resource grouping
  tenantId: "${org}_${user}"                         // ğŸ‘¤ Tenant identifier
};
```

### ğŸŒ Environment Variables

For production deployments, use environment variables for sensitive configuration:

```bash
# ğŸ” API Keys & Credentials
export PINECONE_API_KEY="your_pinecone_key_here"
export COHERE_API_KEY="your_cohere_key_here" 
export GOOGLE_API_KEY="your_google_gemini_key_here"

# â˜ï¸ AWS Configuration
export AWS_ACCESS_KEY_ID="your_aws_access_key"
export AWS_SECRET_ACCESS_KEY="your_aws_secret_key"
export S3_BUCKET_NAME="your_bucket_name"
export AWS_REGION="eu-west-2"

# ğŸ¯ Application Settings
export PINECONE_INDEX_NAME="forcohere"
export PINECONE_ENVIRONMENT="your_pinecone_env"
export PROCESSING_TIMEOUT="300000"
export MAX_CONCURRENT_SESSIONS="10"
```

**ğŸ”’ Security Best Practices:**
- Never commit API keys to version control
- Use different keys for development/staging/production
- Rotate keys regularly
- Monitor API usage and set billing alerts

## ğŸ› ï¸ Troubleshooting Guide

### ğŸš¨ Common Issues & Solutions

<details>
<summary>ğŸ“ <strong>Files Not Being Processed</strong></summary>

**Symptoms:** Files uploaded to S3 but workflow doesn't process them

**Solutions:**
```bash
âœ… Check S3 bucket structure matches required format
âœ… Verify both PDF and Excel files are present in session
âœ… Ensure files are in correct subdirectories:
   ğŸ“„ PDFs in: /documents/
   ğŸ“Š Excel in: /questionnaires/
âœ… Confirm cron trigger is active
âœ… Check S3 bucket permissions for n8n
```

**Debug Steps:**
1. Check workflow execution logs in n8n
2. Verify S3 API response in "S3 API - List uploadsfromchatbot Objects"
3. Ensure XML parsing successful
4. Confirm session grouping logic finds your files
</details>

<details>
<summary>ğŸ—„ï¸ <strong>Vector Database Errors</strong></summary>

**Symptoms:** Pinecone connection or embedding failures

**Solutions:**
```bash
âœ… Confirm Pinecone index exists and is active
âœ… Check embedding model compatibility (Cohere)
âœ… Verify namespace naming follows: org_user_session
âœ… Ensure Pinecone pod has sufficient capacity
âœ… Check API key permissions and quotas
```

**Debug Commands:**
```javascript
// Test Pinecone connection
console.log('Pinecone namespace:', pineconeNamespace);
console.log('Index name:', 'forcohere');
```
</details>

<details>
<summary>ğŸ¤– <strong>Q&A Generation Issues</strong></summary>

**Symptoms:** RAG agent not generating answers or poor quality responses

**Solutions:**
```bash
âœ… Ensure PDF processing completed successfully
âœ… Check Gemini API quota and permissions
âœ… Verify Excel questionnaire format is correct
âœ… Confirm vector embeddings are stored
âœ… Test individual question complexity
```

**Quality Improvement:**
- Reduce chunk size for more precise retrieval
- Increase topK parameter for more context
- Improve question formatting in Excel files
- Check document quality and OCR accuracy
</details>

<details>
<summary>ğŸ” <strong>Authentication Failures</strong></summary>

**Symptoms:** API credential errors or permission denied

**Solutions:**
```bash
âœ… Re-validate all API credentials in n8n
âœ… Check IAM permissions for S3 access
âœ… Verify OAuth2 tokens for Google services
âœ… Ensure API keys haven't expired
âœ… Test credentials outside of n8n
```

**Required Permissions:**
- **S3**: ListBucket, GetObject, PutObject
- **Pinecone**: Query, Upsert, Delete
- **Google**: Generative AI access
</details>

### ğŸ” Debug Mode Activation

Enable comprehensive logging throughout the workflow:

```javascript
// Add to any Code node for detailed debugging
console.log('ğŸ” === DEBUG MODE ENABLED ===');
console.log('ğŸ“Š Input data:', JSON.stringify($json, null, 2));
console.log('ğŸ¯ Processing stage:', 'your_stage_name');
console.log('â° Timestamp:', new Date().toISOString());

// For session tracking
console.log('ğŸ¢ Org:', org);
console.log('ğŸ‘¤ User:', user);  
console.log('ğŸ“Š Session:', session);
console.log('ğŸ—‚ï¸ Namespace:', pineconeNamespace);
```

### Performance Optimization

For large-scale deployments:
- **Increase Worker Memory**: Adjust n8n instance resources
- **Batch Size Tuning**: Reduce `MAX_SESSIONS_PER_BATCH` for stability
- **Cron Frequency**: Decrease monitoring frequency for large buckets
- **Pinecone Scaling**: Use larger Pinecone pod sizes for better performance

## ğŸ“Š Monitoring and Maintenance

### Health Checks
- Monitor n8n execution logs for errors
- Check Pinecone usage and quota
- Verify S3 bucket permissions and costs
- Monitor API rate limits (Cohere, Google, Pinecone)

### Regular Maintenance
- Clean up old result files from S3
- Archive processed sessions
- Update API keys before expiration
- Review and optimize chunk sizes based on performance

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support

For support and questions:
- Create an issue in this repository
- Check the [n8n documentation](https://docs.n8n.io/)
- Review [Pinecone documentation](https://docs.pinecone.io/)
- Consult [LangChain documentation](https://docs.langchain.com/)

---

**Built with â¤ï¸ using n8n, Pinecone, and modern AI technologies**
